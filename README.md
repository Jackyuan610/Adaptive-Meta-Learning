# Adaptive Few-Shot Meta-Learning with Drift Detection and Task-Aware Optimization

This repository contains the official implementation of our paper:

> **"Adaptive Few-Shot Meta-Learning with Drift Detection and Task-Aware Optimization"**  
> Yiming Yuan

## ğŸ¯ Overview

This project implements a novel continual few-shot learning framework that adaptively handles distributional drift through:

- **Transformer-based Task Embedding**: Captures task characteristics using global statistics, class prototypes, and contextual features
- **Hybrid Drift Detection**: Combines KL divergence, mean shift, and MLP classifier for robust drift detection
- **Adaptive Controller**: Dynamically adjusts learning rates and regularization strength based on task complexity and drift signals
- **Conditional Optimization**: Routes between ProtoOnly and MetaSGD+EWC paths based on drift detection

## ğŸ“ Project Structure

```
Design/
â”œâ”€â”€ ğŸ“„ main_ablation.py          # Main training script with ablation studies
â”œâ”€â”€ ğŸ“„ models.py                 # Task embedding composer + ProtoNet learner
â”œâ”€â”€ ğŸ“„ meta_sgd_module.py        # MetaSGD with per-parameter learning rates + EWC
â”œâ”€â”€ ğŸ“„ drift_detector.py         # Hybrid drift detection (KL + Mean Shift + MLP)
â”œâ”€â”€ ğŸ“„ adaptive_controller.py    # Task-aware controller for Î·, Î» adjustment
â”œâ”€â”€ ğŸ“„ hypernet.py              # Gated hypernetwork for hyperparameter modulation
â”œâ”€â”€ ğŸ“„ drift_detection.py        # Task memory and replay buffer
â”œâ”€â”€ ğŸ“„ task_loader.py            # Few-shot task sampler for Mini-ImageNet
â”œâ”€â”€ ğŸ“„ task_stream_simulator.py  # Simulates task stream with controlled drift
â”œâ”€â”€ ğŸ“„ visualization.py          # Task embedding visualization and drift plots
â”œâ”€â”€ ğŸ“„ experiment_utils.py       # Experiment logging and metrics computation
â”œâ”€â”€ ğŸ“„ environment.yml           # Conda environment configuration
â”œâ”€â”€ ğŸ“„ requirements.txt          # Python dependencies
â”œâ”€â”€ ğŸ“ data/                     # Dataset directory
â”‚   â””â”€â”€ mini-imagenet/          # Mini-ImageNet dataset
â”œâ”€â”€ ğŸ“ docs/                     # Documentation and papers
â”‚   â”œâ”€â”€ main.tex                # Main paper
â”‚   â””â”€â”€ references.bib          # Bibliography
â”œâ”€â”€ ğŸ“ experiments/              # Experiment outputs
â””â”€â”€ ğŸ“ logs/                     # Training logs
```

## ğŸ§  Method Architecture

| Component | Module | Paper Section | Description |
|-----------|--------|---------------|-------------|
| **Task Embedding** | `TaskEmbeddingComposer` | III-B | `t_i = [Î¼_i, Ïƒ_i, {p_c}, h_ctx]` |
| **Drift Detection** | `DriftDetector` | III-C | Hybrid: KL + Mean Shift + MLP |
| **Adaptive Controller** | `AdaptiveMetaController` | III-D | Strategy selection and routing |
| **Gated Hypernetwork** | `GatedHyperNetwork` | III-D | Î·_i = Î·_0(1+Î±C_i+Î²D_i), Î»_i = Î»_0(1+Î³D_i) |
| **MetaSGD+EWC** | `MetaSGDModule` | III-E | Î¸' = Î¸ - Î·âˆ‡L + Î»/2 Î£F_ii(Î¸-Î¸*)Â² |
| **Task Memory** | `TaskMemory` | III-F | Replay buffer for catastrophic forgetting |

## ğŸš€ Quick Start

### 1. Environment Setup
```bash
# Option 1: Using conda (recommended)
conda env create -f environment.yml
conda activate cfsl_env

# Option 2: Using pip
pip install -r requirements.txt
```

### 2. Dataset Setup
```bash
# Setup all datasets (Mini-ImageNet, Omniglot)
python setup_datasets.py

# Or setup individually
python setup_datasets.py --dataset mini-imagenet
python setup_datasets.py --dataset omniglot
```

### 3. Run Experiments
```bash
# Run on Mini-ImageNet (default)
python main_ablation.py

# Run on Omniglot
python main_ablation.py --dataset omniglot

# Run ablation studies
python main_ablation.py --disable-controller
python main_ablation.py --disable-drift
python main_ablation.py --disable-ewc
```

### 4. Verify Implementation
```bash
# Verify alignment with paper methodology
python test_experiment_alignment.py
```

## ğŸ“Š Key Features

### Drift Detection
- **KL Divergence**: Measures distributional changes between task embeddings
- **Mean Shift**: Detects gradual drift in embedding space
- **MLP Classifier**: Learns to predict drift from task characteristics
- **Voting Mechanism**: Requires at least 2 signals for drift confirmation

### Adaptive Control
- **Learning Rate**: Î·_i = Î·_0(1 + Î±C_i + Î²D_i) where C_i is task complexity, D_i is drift score
- **Regularization**: Î»_i = Î»_0(1 + Î³D_i) for EWC strength adjustment
- **Strategy Selection**: Routes between ProtoOnly and MetaSGD+EWC based on drift signals

### Conditional Optimization
- **ProtoOnly Path**: Standard prototype learning for stable tasks
- **MetaSGD+EWC Path**: Fast adaptation with regularization for drifting tasks
- **Task Memory**: Replay buffer to prevent catastrophic forgetting

## ğŸ“ˆ Outputs

After training, the following files are generated:

- **Task Embeddings**: `experiments/task_embeddings.json` - Task-by-task embedding and drift logs
- **Metrics**: `experiments/metrics.json` - Performance metrics and ablation results
- **Visualizations**: 
  - `experiments/epoch_*_tsne.png` - Task embedding t-SNE plots
  - `experiments/drift_trend.png` - Drift detection over time
- **Logs**: `logs/training_*.log` - Detailed training logs

## ğŸ”¬ Evaluation Metrics

| Metric | Description |
|--------|-------------|
| `NTA_avg` | New Task Accuracy - Performance on current task |
| `OTR_avg` | Old Task Retention - Performance on replayed tasks |
| `FR_avg` | Forgetting Rate - Accuracy degradation over time |
| `DDA_acc` | Drift Detection Accuracy - Precision of drift detection |
| `CAS_Î·_var` | Learning Rate Variance - Adaptation variability |
| `CAS_Î»_var` | Regularization Variance - EWC strength variability |
| `AT_avg_sec` | Adaptation Time - Average time per task |

## ğŸ“š Citation

```bibtex
@misc{yuan2025adaptivecfsl,
  title={Adaptive Few-Shot Meta-Learning with Drift Detection and Task-Aware Optimization},
  author={Yiming Yuan},
  year={2025},
}
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Mini-ImageNet dataset from [Vinyals et al. (2016)](https://arxiv.org/abs/1606.04080)
- ProtoNet implementation inspired by [Snell et al. (2017)](https://arxiv.org/abs/1703.05175)
- MetaSGD implementation based on [Li et al. (2017)](https://arxiv.org/abs/1707.09835)
