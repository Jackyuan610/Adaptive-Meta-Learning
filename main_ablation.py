"""
main_ablation.py

Main experiment script for continual few-shot learning with adaptive meta-learning, drift detection, and ablation studies.
Implements the closed-loop system described in the paper:
"Adaptive Few-Shot Meta-Learning with Drift Detection and Task-Aware Optimization" (Yuan, 2025)

Key features:
- Supports ProtoNet, MAML, iCaRL baselines and ablations
- Integrates transformer-based task embedding, hybrid drift detection, adaptive controller, and conditional optimization
- Logs all metrics and supports visualization and experiment reproducibility
"""

import os, json, time, argparse
import torch
import torch.nn.functional as F
import torch.nn.utils as utils
import numpy as np
from tqdm import tqdm
from torchvision import transforms, models
from collections import defaultdict
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from torch import Tensor

from models import TaskEmbeddingComposer, ProtoLearner, MAMLClassifier, iCaRLClassifier, MetaSGDWithEWC
from drift_detector import HybridDriftDetector
from adaptive_controller import AdaptiveController
from experiment_utils import ExperimentLogger
from dataset_loader import create_dataloader
from visualization import TaskEmbeddingVisualizer
from replay_memory import ReplayMemory
from test_strategies import SimpleMetaSGD  # ä½¿ç”¨ç®€åŒ–çš„MetaSGDå®ç°

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--disable-ewc", action="store_true", help="Disable EWC regularization.")
    parser.add_argument("--disable-controller", action="store_true", help="Disable adaptive controller (fixed hyperparameters).")
    parser.add_argument("--disable-replay", action="store_true", help="Disable replay buffer.")
    parser.add_argument("--disable-drift", action="store_true", help="Disable drift detection (always use ProtoOnly).")
    parser.add_argument("--dataset", type=str, default="mini-imagenet", 
                       choices=["mini-imagenet", "omniglot", "meta-dataset"],
                       help="Dataset to use for experiments.")
    parser.add_argument("--data-root", type=str, default="data",
                       help="Root directory for datasets.")
    parser.add_argument("--method", type=str, default="protonet",
                       choices=["protonet", "maml", "icarl"],
                       help="Which method to run: protonet, maml, icarl.")
    parser.add_argument("--icarl-memory", type=int, default=2000,
                       help="iCaRL memory size.")
    parser.add_argument("--maml-inner-steps", type=int, default=5,
                       help="MAML inner loop steps.")
    parser.add_argument("--backbone", type=str, default="resnet34",
                       choices=["resnet18", "resnet50", "resnet101", "resnet34"],
                       help="Which backbone to use.")
    return parser.parse_args()

def preprocess_images(images):
    """Preprocess images to be compatible with ResNet18"""
    # Define preprocessing transforms
    transform = transforms.Compose([
        transforms.Resize((224, 224)),  # ResNet18 expects 224x224
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # Handle different input formats
    if images.dim() == 3:  # [C, H, W]
        images = images.unsqueeze(0)  # Add batch dimension
    elif images.dim() == 2:  # [H, W] - grayscale
        images = images.unsqueeze(0).repeat(3, 1, 1)  # Convert to RGB
        images = images.unsqueeze(0)  # Add batch dimension
    
    # Ensure we have batch dimension and 3 channels
    if images.size(1) == 1:  # Grayscale
        images = images.repeat(1, 3, 1, 1)
    
    # Apply transforms
    processed_images = []
    for i in range(images.size(0)):
        img = images[i]
        processed_img = transform(img)
        processed_images.append(processed_img)
    
    return torch.stack(processed_images)

def main(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Map dataset names from user input to internal names
    dataset_name_mapping = {
        'mini-imagenet': 'mini_imagenet',
        'omniglot': 'omniglot',
        'meta-dataset': 'meta_dataset'
    }
    internal_dataset_name = dataset_name_mapping.get(args.dataset, args.dataset)
    if internal_dataset_name is None:
        internal_dataset_name = args.dataset

    # Create dataset loader
    dataset_config = {
        'mini_imagenet': os.path.join(args.data_root, 'mini-imagenet'),
        'omniglot': os.path.join(args.data_root, 'omniglot'),
        'meta_dataset': os.path.join(args.data_root, 'meta-dataset')
    }
    dataloader = create_dataloader(dataset_config)
    print(f"\U0001F4CA Using dataset: {args.dataset}")
    
    # Note: Current Mini-ImageNet test split has 20 classes, while paper may use 100 classes
    # This is a common variation in Mini-ImageNet splits across different implementations
    print(f"ğŸ“Š Dataset info: Mini-ImageNet test split has 20 classes (paper may use 100 classes)")
    
    # Task loader
    task_loader = dataloader

    # Enhanced backbone for better performance
    if args.backbone == "resnet18":
        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        feature_dim = 512
    elif args.backbone == "resnet50":
        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        feature_dim = 2048
    elif args.backbone == "resnet101":
        resnet = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)
        feature_dim = 2048
    else:
        resnet = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)
        feature_dim = 512
    
    # Remove final layer and freeze backbone
    resnet.fc = torch.nn.Identity()  # type: ignore
    resnet.to(device).eval()
    
    # Enhanced models with better architecture
    embedder_core = TaskEmbeddingComposer(input_dim=feature_dim, hidden_dim=256).to(device)
    
    # Enhanced ProtoNet with feature normalization
    class EnhancedProtoNet(torch.nn.Module):
        def __init__(self, feature_dim=512, n_way=5):
            super().__init__()
            self.feature_dim = feature_dim
            self.n_way = n_way
            self.feature_norm = torch.nn.LayerNorm(feature_dim)
            
        def forward(self, support_features, support_labels, query_features):
            # Normalize features for better distance computation
            support_features = self.feature_norm(support_features)
            query_features = self.feature_norm(query_features)
            
            # Compute prototypes with better aggregation
            prototypes = []
            for i in range(self.n_way):
                mask = (support_labels == i)
                if mask.sum() > 0:
                    class_features = support_features[mask]
                    # Use weighted average for better prototypes
                    weights = F.softmax(torch.norm(class_features, dim=1), dim=0)
                    prototype = torch.sum(class_features * weights.unsqueeze(1), dim=0)
                    prototypes.append(prototype)
                else:
                    prototypes.append(torch.zeros(self.feature_dim, device=support_features.device))
            
            prototypes = torch.stack(prototypes)
            
            # Use cosine similarity instead of Euclidean distance
            prototypes_norm = F.normalize(prototypes, p=2, dim=1)
            query_norm = F.normalize(query_features, p=2, dim=1)
            
            # Compute cosine similarity
            similarity = torch.mm(query_norm, prototypes_norm.t())
            
            return similarity
    
    proto = EnhancedProtoNet(feature_dim=feature_dim, n_way=5).to(device)

    # Baseline models
    maml_model = MAMLClassifier(resnet, n_way=5, inner_steps=5, inner_lr=0.01).to(device)
    icarl_model = iCaRLClassifier(resnet, memory_size=2000, n_classes=100).to(device)
    
    # MetaSGD for adaptive strategy with enhanced EWC
    metasgd_model = MetaSGDWithEWC(
        feature_extractor=resnet, 
        n_way=5, 
        inner_steps=15,  # å¢åŠ å†…å¾ªç¯æ­¥æ•°
        inner_lr=0.005,  # é™ä½å†…å¾ªç¯å­¦ä¹ ç‡
        lambda_ewc=25.0  # é€‚ä¸­çš„EWCæƒé‡
    ).to(device)

    # Drift detector, controller, logger, visualizer
    drift_detector = HybridDriftDetector(embedding_dim=512)
    controller = AdaptiveController(embedding_dim=512)
    logger = ExperimentLogger(log_dir="experiments", experiment_name="cfsl_ablation")
    viz_manager = TaskEmbeddingVisualizer()

    # Initialize replay memory for drift detection and retention (Section 3.4 of paper)
    replay_memory = ReplayMemory(max_size=20)  # Paper standard: 20 tasks in memory
    
    # Main continual learning loop - Paper standard: 100 tasks
    total_tasks = 100  # Paper standard: 100 tasks for full experiment
    drift_scores, controller_params, strategies, accuracies, task_ids = [], [], [], [], []
    
    # Strategy performance tracking
    strategy_performance = {"ProtoNet": [], "MetaSGD": []}
    
    # Additional metrics as per paper Section 5.1
    nta_scores = []  # New Task Accuracy
    otr_scores = []  # Old Task Retention  
    fr_scores = []   # Forgetting Rate
    dda_scores = []  # Drift Detection Accuracy
    cas_scores = []  # Controller Adjustment Score
    adaptation_times = []  # Adaptation Time
    
    # Drift injection schedule (every 10 tasks as per paper Section 5.1)
    drift_injection_points = [i for i in range(10, total_tasks, 10)]  # Tasks 10, 20, 30, ..., 90

    # ä¼˜åŒ–è®­ç»ƒå‚æ•°
    n_way = 5
    n_support = 5  # å¢åŠ supportæ ·æœ¬æ•°
    n_query = 15   # å¢åŠ queryæ ·æœ¬æ•°
    n_tasks = 100
    n_inner_tasks = 8  # å¢åŠ å†…å¾ªç¯ä»»åŠ¡æ•°
    n_inner_steps = 5  # å¢åŠ å†…å¾ªç¯æ­¥æ•°
    meta_lr = 0.01
    task_lr = 0.1

    for task_idx in tqdm(range(total_tasks), desc="Tasks"):
        # Sample task as per paper: 5-way 5-shot with 15 query samples
        task = dataloader.sample_task(internal_dataset_name, n_way=5, n_support=5, n_query=15)
        
        # Get support and query data
        s_imgs, s_labels = task.get_support_tensors()
        q_imgs, q_labels = task.get_query_tensors()
        
        # Move to device
        s_imgs, s_labels = s_imgs.to(device), s_labels.to(device)
        q_imgs, q_labels = q_imgs.to(device), q_labels.to(device)
        
        # Extract features using frozen backbone (as per paper)
        with torch.no_grad():
            s_feats = resnet(s_imgs)
            q_feats = resnet(q_imgs)
        
        # Task embedding construction (Section 3.1 of paper)
        task_embedding = embedder_core(s_feats, s_labels).mean(dim=0)
        
        # å…ˆåŠ å…¥å†å²å†æ£€æµ‹æ¼‚ç§»
        drift_detector.add_embedding(task_embedding)
        drift_score, mlp_score = drift_detector.detect_drift(task_embedding)
        
        # ä»æ¼‚ç§»æ£€æµ‹å™¨è·å–æ‰€æœ‰ä¸‰ä¸ªä¿¡å·
        # æ³¨æ„ï¼šdrift_detector.detect_drift()å†…éƒ¨å·²ç»è®¡ç®—äº†ä¸‰ä¸ªä¿¡å·ï¼Œä½†æˆ‘ä»¬éœ€è¦åœ¨ä¸»ç¨‹åºä¸­é‡æ–°è®¡ç®—
        # ä»¥ç¡®ä¿ä¸è®ºæ–‡çš„å¤šæ•°æŠ•ç¥¨æœºåˆ¶ä¸€è‡´
        
        # Adaptive controller (Section 3.3 of paper)
        adaptive_params = controller.compute_adaptive_parameters(task_embedding, float(drift_score), task_idx)
        
        # Strategy selection based on drift detection - aligned with paper Algorithm 3
        # æŒ‰ç…§è®ºæ–‡è®¾ç½®é˜ˆå€¼
        drift_threshold = 0.15  # è®ºæ–‡æ ‡å‡†é˜ˆå€¼
        mlp_threshold = 0.5     # MLPåˆ†ç±»å™¨é˜ˆå€¼
        mean_shift_threshold = 0.1  # å‡å€¼æ¼‚ç§»é˜ˆå€¼
        
        # ç¡®ä¿åˆ†æ•°æ˜¯floatç±»å‹
        drift_score_float = float(drift_score) if isinstance(drift_score, (int, float)) else 0.0
        mlp_score_float = float(mlp_score) if isinstance(mlp_score, (int, float)) else 0.0
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"  ğŸ” Drift Debug: score={drift_score_float:.3f}, mlp={mlp_score_float:.3f}, threshold={drift_threshold}")
        
        # ç­–ç•¥é€‰æ‹©é€»è¾‘ - æŒ‰ç…§è®ºæ–‡çš„å¤šæ•°æŠ•ç¥¨æœºåˆ¶
        if task_idx == 0:
            # ç¬¬ä¸€ä¸ªä»»åŠ¡ï¼Œä½¿ç”¨ProtoNet
            strategy = 'ProtoNet'
            print(f"  ğŸš€ First task, using ProtoNet")
        elif task_idx < 3:
            # å‰3ä¸ªä»»åŠ¡éƒ½ä½¿ç”¨ProtoNetï¼Œå»ºç«‹åŸºç¡€
            strategy = 'ProtoNet'
            print(f"  ğŸ—ï¸ Early task ({task_idx+1}), using ProtoNet for foundation")
        else:
            # æŒ‰ç…§è®ºæ–‡çš„å¤šæ•°æŠ•ç¥¨æœºåˆ¶ - ä½¿ç”¨æ‰€æœ‰ä¸‰ä¸ªä¿¡å·
            # è®¡ç®—å„ä¸ªæ£€æµ‹å™¨çš„ä¿¡å·
            kl_signal = 1.0 if drift_score_float > drift_threshold else 0.0
            mlp_signal = 1.0 if mlp_score_float > mlp_threshold else 0.0
            # å‡å€¼æ¼‚ç§»ä¿¡å· - ä»drift_scoreä¸­æå–ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            mean_shift_signal = 1.0 if drift_score_float > mean_shift_threshold else 0.0
            
            # å¤šæ•°æŠ•ç¥¨ï¼šè‡³å°‘2ä¸ªæ£€æµ‹å™¨è§¦å‘ï¼ˆæŒ‰ç…§è®ºæ–‡è¦æ±‚ï¼‰
            signals = [kl_signal, mlp_signal, mean_shift_signal]
            drift_detected = sum(signals) >= 2  # è‡³å°‘2ä¸ªæ£€æµ‹å™¨è§¦å‘
            
            if drift_detected:
                strategy = 'MetaSGD'
                print(f"  ğŸ”„ Drift detected by majority voting (KL={kl_signal}, MLP={mlp_signal}, MeanShift={mean_shift_signal}), switching to MetaSGD")
            else:
                strategy = 'ProtoNet'
                print(f"  âœ… No drift detected (KL={kl_signal}, MLP={mlp_signal}, MeanShift={mean_shift_signal}), using ProtoNet")
        
        # Strategy selection: Low drift -> ProtoNet (fast), High drift -> MetaSGD (adaptive)
        # ä¿®å¤ç­–ç•¥é€‰æ‹©é€»è¾‘ï¼šç¡®ä¿å½“æ¼‚ç§»åˆ†æ•°è¶…è¿‡é˜ˆå€¼æ—¶é€‰æ‹©MetaSGD
        if strategy == "MetaSGD":
            # MetaSGD + EWC path: Perform inner-loop adaptation with regularization
            metasgd_model.update_ewc_info(s_imgs, s_labels)  # ä¼ å…¥åŸå§‹å›¾åƒï¼Œä¸æ˜¯ç‰¹å¾
            
            # å¢åŠ å†…å¾ªç¯æ­¥æ•°
            for inner_step in range(15):  # ä»10æ­¥å¢åŠ åˆ°15æ­¥
                logits = metasgd_model(s_imgs, s_labels, q_imgs, adaptive_params)  # ä¼ å…¥åŸå§‹å›¾åƒ
                loss = F.cross_entropy(logits, q_labels)
                
                # æ·»åŠ EWCæ­£åˆ™åŒ–æŸå¤±
                ewc_loss = metasgd_model.compute_ewc_loss()
                ewc_weight = adaptive_params.get('ewc_weight', 0.1)
                if isinstance(ewc_weight, (int, float)):
                    total_loss = loss + ewc_weight * ewc_loss
                else:
                    total_loss = loss + 0.1 * ewc_loss
                
                # æ·»åŠ æ ‡ç­¾å¹³æ»‘å’Œæ¸©åº¦ç¼©æ”¾
                temperature = 0.05  # é™ä½æ¸©åº¦
                smoothed_logits = logits / temperature
                total_loss = total_loss + 0.1 * F.kl_div(
                    F.log_softmax(smoothed_logits, dim=1), 
                    F.softmax(torch.randn_like(smoothed_logits), dim=1), 
                    reduction='batchmean'
                )
        else:
            # ProtoNet path: Fast prototype-based inference only
            logits = proto(s_feats, s_labels, q_feats)
            loss = F.cross_entropy(logits, q_labels)
            total_loss = loss
        
        # Compute accuracy
        accuracy = (logits.argmax(1) == q_labels).float().mean().item()
        
        # Track strategy performance
        strategy_performance[strategy].append(accuracy)
        
        # Store metrics as per paper Section 5.1
        drift_scores.append(drift_score_float)
        controller_params.append(adaptive_params)
        strategies.append(strategy)
        accuracies.append(accuracy)
        task_ids.append(task_idx)
        
        # Update memory for replay (Section 3.4 of paper)
        replay_memory.update(task_embedding, task_idx)
        
        # Enhanced replay mechanism for better accuracy (Section 3.4 of paper)
        if replay_memory.size() > 5 and task_idx % 3 == 0:  # Replay every 3 tasks
            # Sample historical tasks for replay
            replay_embeddings, replay_ids = replay_memory.sample_tasks(num_samples=3)
            
            if replay_embeddings and strategy == "MetaSGD":  # Only replay for MetaSGD path
                # Replay training: reinforce knowledge from past tasks
                replay_loss = 0.0
                for replay_embedding in replay_embeddings:
                    replay_embedding = replay_embedding.to(device)
                    
                    # Replay with MetaSGD to reinforce adaptation
                    replay_logits = metasgd_model(s_imgs, s_labels, q_imgs)
                    replay_loss += F.cross_entropy(replay_logits, q_labels)
                
                replay_loss /= len(replay_embeddings)
                
                # Add replay loss to total loss (weighted by replay_weight from controller)
                replay_weight = adaptive_params.get('replay_weight', 0.5)
                if isinstance(replay_weight, (int, float)):
                    total_loss = total_loss + replay_weight * replay_loss
                else:
                    total_loss = total_loss + 0.5 * replay_loss
                
                print(f"  ğŸ”„ Replay training applied (weight: {replay_weight:.2f})")
            else:
                total_loss = loss
        else:
            total_loss = loss
        
        # Log progress
        print(f"[Task {task_idx+1}] Acc: {accuracy:.2%}, Loss: {total_loss:.4f}, "
              f"Drift: {drift_score_float:.3f}, Strategy: {strategy}")
        
        # Train MLP classifier based on strategy selection
        if task_idx >= 3:  # Start training after early tasks
            # ä¿®å¤MLPè®­ç»ƒé€»è¾‘ - æŒ‰ç…§è®ºæ–‡ä½¿ç”¨ä¼ªæ ‡ç­¾
            # æŒ‰ç…§è®ºæ–‡ï¼šä½¿ç”¨å‰ä¸¤ä¸ªä¿¡å·çš„ä¼ªæ ‡ç­¾
            kl_signal = 1.0 if drift_score_float > drift_threshold else 0.0
            mlp_signal = 1.0 if mlp_score_float > mlp_threshold else 0.0
            pseudo_label = bool((kl_signal + mlp_signal) >= 2)  # è®ºæ–‡çš„ä¼ªæ ‡ç­¾é€»è¾‘ï¼Œè½¬æ¢ä¸ºbool
            drift_detector.train_mlp_classifier(task_embedding, pseudo_label)

    # Save logs and visualize
    logger.save_logs()
    viz_manager.plot_drift_trend(drift_scores, task_ids, save_name="experiments/drift_trend.png")
    viz_manager.plot_controller_parameters(controller_params, task_ids, save_name="experiments/controller_params.png")
    viz_manager.plot_strategy_switches(strategies, task_ids, save_name="experiments/strategy_switches.png")
    results = {"ProtoNet": accuracies}
    viz_manager.plot_performance_comparison(results, task_ids, save_name="experiments/performance_comparison.png")
    
    # Strategy performance summary
    print("\n" + "="*60)
    print("ğŸ“Š Strategy Performance Summary:")
    for strategy, perfs in strategy_performance.items():
        if perfs:
            avg_acc = np.mean(perfs)
            std_acc = np.std(perfs)
            count = len(perfs)
            print(f"{strategy}: {avg_acc:.2%} Â± {std_acc:.2%} ({count} tasks)")
        else:
            print(f"{strategy}: No tasks used")
    
    print("\n\U0001F389 Experiment completed! Results and plots saved.")

if __name__ == "__main__":
    args = parse_args()
    main(args)
